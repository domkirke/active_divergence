{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118c128f",
   "metadata": {},
   "source": [
    "# Generating with VAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cafb0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/domkirke/anaconda3/lib/python3.7/site-packages/nsgt/fft.py:116: UserWarning: nsgt.fft falling back to numpy.fft\n",
      "  warn(\"nsgt.fft falling back to numpy.fft\")\n"
     ]
    }
   ],
   "source": [
    "from active_divergence import models\n",
    "import pytorch_lightning as pl\n",
    "import torch, torch.nn as nn, torch.distributions as dist\n",
    "import re, os, torch, torchaudio, random, argparse, pdb, einops, tqdm, numpy as np, dill\n",
    "from active_divergence import models, hack\n",
    "from active_divergence.utils import checkdir, checklist\n",
    "from active_divergence.data.audio.dataset import parse_audio_file, AudioDataset\n",
    "from active_divergence.data.audio import parse_transforms, AudioTransform\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def get_file_names(file_paths):\n",
    "    file_paths = [file_paths] if os.path.isfile(file_paths) else [f\"{file_paths}/{f}\" for f in os.listdir(file_paths)]\n",
    "    file_paths = list(filter(lambda x: x[0] != \".\" and os.path.splitext(x)[1] in AudioDataset.types, file_paths))\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f85e511",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6e90ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"configs/audio/vae/additive_vae.yaml\"\n",
    "model_path = \"/Volumes/Canard/models/additive-vae-mse/last.ckpt\"\n",
    "transform_path = \"/Volumes/Canard/models/additive-vae-mse/transforms.ct\"\n",
    "out_path = \"samples/additive_vae\"\n",
    "device_id = -1\n",
    "hack_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef2265b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['encoder.pre_conv.weight', 'encoder.pre_conv.bias', 'encoder.conv_modules.0.conv.weight', 'encoder.conv_modules.0.norm.weight', 'encoder.conv_modules.0.norm.bias', 'encoder.conv_modules.1.conv.weight', 'encoder.conv_modules.1.norm.weight', 'encoder.conv_modules.1.norm.bias', 'encoder.conv_modules.2.conv.weight', 'encoder.conv_modules.2.norm.weight', 'encoder.conv_modules.2.norm.bias', 'encoder.flatten_module.1.module.0.linear.weight', 'encoder.flatten_module.1.module.0.linear.bias', 'encoder.flatten_module.1.module.0.norm.weight', 'encoder.flatten_module.1.module.0.norm.bias', 'encoder.flatten_module.1.module.1.linear.weight', 'encoder.flatten_module.1.module.1.linear.bias', 'decoder.conv_modules.0.conv.weight', 'decoder.conv_modules.1.conv.weight', 'decoder.conv_modules.2.conv.weight', 'decoder.final_conv.weight', 'decoder.final_conv.bias', 'decoder.flatten_module.0.module.0.linear.weight', 'decoder.flatten_module.0.module.0.linear.bias', 'decoder.flatten_module.0.module.0.norm.weight', 'decoder.flatten_module.0.module.0.norm.bias', 'decoder.flatten_module.0.module.1.linear.weight', 'decoder.flatten_module.0.module.1.linear.bias'])\n",
      "ComposeAudioTransform(['Mono(mode=mix, squeeze=1, normalize=False, dim=-2, invert_as_stereo=True)\\n', 'STFT(nfft=1024, hop_size=256, sr=44100, backend=tifresi)\\n', '<active_divergence.data.audio.transforms.Magnitude object at 0x13dbc5e90>\\n', 'Unsqueeze(dim=-2)\\n'])\n"
     ]
    }
   ],
   "source": [
    "# Set up model\n",
    "config = OmegaConf.load(config_path)\n",
    "model_type = getattr(models, config.model.type)\n",
    "device = torch.device('cpu') if device_id < 0 else torch.device(\"cuda:%s\"%device_id)\n",
    "\n",
    "# Load model\n",
    "source_model = model_type.load_from_checkpoint(model_path, map_location=device)\n",
    "print(dict(source_model.named_parameters()).keys())\n",
    "source_model.to(device)\n",
    "if transform_path is None:\n",
    "    pre_transforms = parse_transforms(config.data.transforms.get('pre_transforms', AudioTransform()))\n",
    "    transforms = parse_transforms(config.data.transforms.transforms)\n",
    "    transforms = pre_transforms+transforms\n",
    "else:\n",
    "    with open(transform_path, \"rb\") as f:\n",
    "        transforms = dill.load(f)\n",
    "        \n",
    "# Hack\n",
    "model = source_model\n",
    "hack_path = \"samples/additive_vae/hack_additive.yaml\"\n",
    "hack_config = OmegaConf.load(hack_path)\n",
    "if hack_model:\n",
    "    hack.hack_model(model, hack_config)\n",
    "    hack.hook_model(model, hack_config)\n",
    "model.eval()\n",
    "print(transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f59ccc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transfering key decoder.conv_modules.0.conv.weight\n",
      "transfering key decoder.conv_modules.1.conv.weight\n",
      "transfering key decoder.conv_modules.2.conv.weight\n",
      "transfering key decoder.final_conv.weight\n",
      "transfering key decoder.final_conv.bias\n",
      "transfering key decoder.flatten_module.0.module.0.linear.weight\n",
      "transfering key decoder.flatten_module.0.module.0.linear.bias\n",
      "transfering key decoder.flatten_module.0.module.0.norm.weight\n",
      "transfering key decoder.flatten_module.0.module.0.norm.bias\n",
      "transfering key decoder.flatten_module.0.module.0.norm.running_mean\n",
      "transfering key decoder.flatten_module.0.module.0.norm.running_var\n",
      "transfering key decoder.flatten_module.0.module.0.norm.num_batches_tracked\n",
      "transfering key decoder.flatten_module.0.module.1.linear.weight\n",
      "transfering key decoder.flatten_module.0.module.1.linear.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transfer from another model\n",
    "import re\n",
    "\n",
    "keys = ['decoder(.*)']\n",
    "model_path_alt = \"/Volumes/Canard/models/fm-vae-mse/last.ckpt\"\n",
    "transfer_model = model_type.load_from_checkpoint(model_path, map_location=device)\n",
    "model_dict = source_model.state_dict()\n",
    "transfer_dict = transfer_model.state_dict()\n",
    "\n",
    "for k, v in model_dict.items():\n",
    "    for target_key in keys:\n",
    "        if re.match(target_key, k):\n",
    "            if not k in transfer_dict.keys():\n",
    "                print('key %s not found'%k)\n",
    "            if transfer_dict[k].shape != v.shape:\n",
    "                print('key %s does not match : original %s, transfered %s'%(v.shape, transfer_dict[k].shape))\n",
    "            model_dict[k] = transfer_dict[k]\n",
    "            print('transfering key %s'%k)\n",
    "\n",
    "model.load_state_dict(model_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650a579b",
   "metadata": {},
   "source": [
    "## Forwarding samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10c78e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples/additive_vae/forward/ressemble.wav => torch.Size([2, 162816])\n"
     ]
    }
   ],
   "source": [
    "def forward_file(model, f, config, sample=False):\n",
    "    x, sr = parse_audio_file(f, sr=config.get('sr'), bitrate=config.get('bitrate'))\n",
    "    x = transforms(x).float()\n",
    "    original, generation = model.reconstruct(x)\n",
    "    if isinstance(generation, dist.Distribution):\n",
    "        generation = generation.sample() if sample else generation.mean\n",
    "    return generation\n",
    "\n",
    "forward_path = f\"{out_path}/forward\"\n",
    "checkdir(forward_path)\n",
    "file_paths = \"samples/additive_vae/src\"\n",
    "file_paths = [file_paths] if os.path.isfile(file_paths) else [f\"{file_paths}/{f}\" for f in os.listdir(file_paths)]\n",
    "file_paths = list(filter(lambda x: x[0] != \".\" and os.path.splitext(x)[1] in AudioDataset.types, file_paths))\n",
    "sample_generation = False\n",
    "\n",
    "for f in file_paths:\n",
    "    with torch.no_grad():\n",
    "        out = forward_file(model, f, config.data, sample=sample_generation)\n",
    "    filename = os.path.splitext(os.path.basename(f))[0]\n",
    "    current_out_path = f\"{forward_path}/{filename}.wav\"\n",
    "    out_raw = transforms.invert(out)\n",
    "    if out_raw.ndim < 1:\n",
    "        out_raw = out_raw.unsqueeze(0)\n",
    "    print(current_out_path, \"=>\", out_raw.shape)\n",
    "    torchaudio.save(current_out_path, out_raw, sample_rate=config.get('sr', 44100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac769a58",
   "metadata": {},
   "source": [
    "## Sample from trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb4e429a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import trajectories as tj\n",
    "import numpy as np\n",
    "\n",
    "sample_generations = False\n",
    "\n",
    "tj.GLOBAL_DIM = model.latent.dim\n",
    "n_batches = 10\n",
    "traj_path = f\"{out_path}/trajectories\"\n",
    "checkdir(traj_path)\n",
    "\n",
    "# Setup trajectories\n",
    "t_range = np.array([0., 1.])\n",
    "t_step = 0.0005\n",
    "timescales = {'x1' : np.array([np.arange(0., 1., t_step)] * n_batches),\n",
    "     'x2' : np.array([np.arange(0., 1., t_step*2)] * n_batches),\n",
    "     'x5' : np.array([np.arange(0., 1., t_step*5)] * n_batches),\n",
    "     'x0.5' : np.array([np.arange(0., 1., t_step*0.5)] * n_batches),\n",
    "     'x0.25' : np.array([np.arange(0., 1., t_step*0.25)] * n_batches)}\n",
    "trajectories = {\n",
    "    \"line\": tj.Line_(t_range, [np.random.randn(n_batches, tj.GLOBAL_DIM), np.random.randn(n_batches, tj.GLOBAL_DIM)]),\n",
    "    \"circle\": tj.Circle_(t_range, radius=np.random.uniform(0.1, 5.0, size=(n_batches,))),\n",
    "    \"saw\": tj.Sawtooth_(freq=1.0, phase=tj.uniform(tj.GLOBAL_DIM), amplitude=np.random.uniform(0.1, 5.0, size=(n_batches,))),\n",
    "    \"sin\": tj.Square_(freq=1.0, phase=tj.uniform(tj.GLOBAL_DIM), amplitude=np.random.uniform(0.1, 5.0, size=(n_batches,))),\n",
    "    \"square\": tj.Square_(freq=1.0, phase=tj.uniform(tj.GLOBAL_DIM), amplitude=np.random.uniform(0.1, 5.0, size=(n_batches,)))    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0e66133",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sn/3rsf7h6j1cd46_rxqs61nm_00000gn/T/ipykernel_28624/2877287322.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_traj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mradius\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mgenerations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_traj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mgenerations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msample_generations\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mgenerations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/code/active_divergence/active_divergence/models/auto_encoders.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfull_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/code/active_divergence/active_divergence/modules/encoders.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mod, transition, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m             \u001b[0;31m# add to previous outputs for residual connections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"residual\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/code/active_divergence/active_divergence/modules/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;34m\"\"\"Performs convolution.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeconvLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGatedConvLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConvLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/code/active_divergence/active_divergence/modules/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mod_closure, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"activation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36melu\u001b[0;34m(input, alpha, inplace)\u001b[0m\n\u001b[1;32m   1389\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1391\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1392\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for k, v in trajectories.items():\n",
    "    for k_t, t in timescales.items():\n",
    "        checkdir(f\"{traj_path}/{k_t}\")\n",
    "        try:\n",
    "            sampled_traj = torch.from_numpy(v(t)).float()\n",
    "            if sampled_traj.isnan().any():\n",
    "                print(sampled_traj, v.radius)\n",
    "            with torch.no_grad():\n",
    "                generations = model.decode(sampled_traj)\n",
    "            if isinstance(generations, dist.Normal):\n",
    "                generations = generations.sample() if sample_generations else generations.mean\n",
    "            for i, g in enumerate(generations):\n",
    "                g = transforms.invert(g.squeeze())\n",
    "                filename = f\"{traj_path}/{k_t}/{k}_{i}.wav\"\n",
    "                torchaudio.save(filename, g, sample_rate=config.get('sr', 44100))\n",
    "        except Exception as e:\n",
    "            print('Error with trajectory %s'%k)\n",
    "            raise e\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ec1b4a",
   "metadata": {},
   "source": [
    "## Time stretching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5dfbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trajectories as tj\n",
    "trajectories.GLOBAL_DIM = config.model.latent.dim\n",
    "\n",
    "def get_trajectory(model, f, config, sample=False):\n",
    "    x, sr = parse_audio_file(f, sr=config.get('sr'), bitrate=config.get('bitrate'))\n",
    "    x = transforms(x).float()\n",
    "    with torch.no_grad():\n",
    "        generation = model.encode(x)\n",
    "    if isinstance(generation, dist.Distribution):\n",
    "        generation = generation.sample() if sample else generation.mean\n",
    "    return generation\n",
    "\n",
    "def get_generation(model, z, config, sample=False):\n",
    "    with torch.no_grad():\n",
    "        out = model.decode(torch.from_numpy(z).float())\n",
    "    if isinstance(out, dist.Distribution):\n",
    "        out = out.sample() if sample_generations else out.mean\n",
    "    return out\n",
    "\n",
    "stretching_path = f\"{out_path}/stretch\"\n",
    "checkdir(stretching_path)\n",
    "file_paths = \"samples/additive_vae/src\"\n",
    "file_paths = get_file_names(file_paths)\n",
    "sample_latent = False\n",
    "sample_generations = False\n",
    "t_range = np.array([0., 1.])\n",
    "t_factor = [2, 4]\n",
    "\n",
    "for f in file_paths:\n",
    "    filename = os.path.splitext(os.path.basename(f))[0]\n",
    "    # obtain trajectory\n",
    "    traj = get_trajectory(model, f, config.data, sample=sample_latent)\n",
    "    traj = traj.numpy()\n",
    "    n_steps = traj.shape[0]\n",
    "    for t_f in t_factor:\n",
    "        t = np.linspace(0., 1., int(n_steps * t_f))\n",
    "        interp = tj.Interpolation_(t_range=t_range, trajectory=traj)(t)\n",
    "        out = get_generation(model, interp, config.data, sample=sample_generations)\n",
    "        # export\n",
    "        current_out_path = f\"{stretching_path}/{filename}_{t_f}.wav\"\n",
    "        out_raw = transforms.invert(out)\n",
    "        if out_raw.ndim < 1:\n",
    "            out_raw = out_raw.unsqueeze(0)\n",
    "        print(current_out_path, \"=>\", out_raw.shape)\n",
    "        torchaudio.save(current_out_path, out_raw, sample_rate=config.get('sr', 44100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b3e5a7",
   "metadata": {},
   "source": [
    "## Interpolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e6e0277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples/additive_vae/interp_src/1/alchimique.wav\n",
      "samples/additive_vae/interp_src/1/architecture.wav\n",
      "samples/additive_vae/interp_src/1/interp_0.wav => torch.Size([2, 306176])\n",
      "samples/additive_vae/interp_src/1/interp_1.wav => torch.Size([2, 306176])\n",
      "samples/additive_vae/interp_src/1/interp_2.wav => torch.Size([2, 306176])\n",
      "samples/additive_vae/interp_src/1/interp_3.wav => torch.Size([2, 306176])\n",
      "samples/additive_vae/interp_src/1/interp_4.wav => torch.Size([2, 306176])\n",
      "architecture.wav\n"
     ]
    }
   ],
   "source": [
    "import trajectories as tj\n",
    "tj.GLOBAL_DIM = config.model.latent.dim\n",
    "\n",
    "def get_trajectory(model, f, config, sample=False):\n",
    "    x, sr = parse_audio_file(f, sr=config.get('sr'), bitrate=config.get('bitrate'))\n",
    "    x = transforms(x).float()\n",
    "    with torch.no_grad():\n",
    "        generation = model.encode(x)\n",
    "    if isinstance(generation, dist.Distribution):\n",
    "        generation = generation.sample() if sample else generation.mean\n",
    "    return generation\n",
    "\n",
    "def get_generation(model, z, config, sample=False):\n",
    "    with torch.no_grad():\n",
    "        out = model.decode(torch.from_numpy(z).float())\n",
    "    if isinstance(out, dist.Distribution):\n",
    "        out = out.sample() if sample_generations else out.mean\n",
    "    return out\n",
    "\n",
    "stretching_path = f\"{out_path}/stretch\"\n",
    "checkdir(stretching_path)\n",
    "file_paths = \"samples/additive_vae/interp_src\"\n",
    "sample_latent = False\n",
    "sample_generations = False\n",
    "n_interp = 5\n",
    "\n",
    "folders = list(filter(lambda x: os.path.isdir(f\"{file_paths}/{x}\"), os.listdir(file_paths)))\n",
    "folders = [f\"{file_paths}/{f}\" for f in folders]\n",
    "stretch_mode = \"max\"\n",
    "\n",
    "trajectories = []\n",
    "anchors = []\n",
    "\n",
    "for folder in folders:\n",
    "    interp_config = OmegaConf.load(f\"{folder}/interp.yaml\")\n",
    "    for file, anchor in interp_config.items():\n",
    "        file_path = f\"{folder}/{file}\"\n",
    "        print(file_path)\n",
    "        trajectories.append(get_trajectory(model, file_path, config.data))\n",
    "        anchors.append(anchor)\n",
    "    target_shape = max(t.shape[0] for t in trajectories)\n",
    "    # stretch trajectories\n",
    "    for i, traj in enumerate(trajectories):\n",
    "        t = np.linspace(0., 1., int(target_shape))\n",
    "        interp = tj.Interpolation_(t_range=t_range, trajectory=traj.numpy())(t)\n",
    "        trajectories[i] = interp\n",
    "    # interpolate\n",
    "    t = np.linspace(0., 1., n_interp)\n",
    "    traj = tj.Morphing_(trajectories=trajectories, anchors=anchors)(t)\n",
    "    for j, current_traj in enumerate(traj):\n",
    "        out = get_generation(model, current_traj, config.data, sample=sample_generations)\n",
    "        # export\n",
    "        current_out_path = f\"{folder}/interp_{j}.wav\"\n",
    "        out_raw = transforms.invert(out)\n",
    "        if out_raw.ndim < 1:\n",
    "            out_raw = out_raw.unsqueeze(0)\n",
    "        print(current_out_path, \"=>\", out_raw.shape)\n",
    "        torchaudio.save(current_out_path, out_raw, sample_rate=config.get('sr', 44100))\n",
    "\n",
    "\"\"\"\n",
    "for f in file_paths:\n",
    "    filename = os.path.splitext(os.path.basename(f))[0]\n",
    "    # obtain trajectory\n",
    "    traj = get_trajectory(model, f, config.data, sample=sample_latent)\n",
    "    traj = traj.numpy()\n",
    "    n_steps = traj.shape[0]\n",
    "    for t_f in t_factor:\n",
    "        t = np.linspace(0., 1., int(n_steps * t_f))\n",
    "        interp = tj.Interpolation_(t_range=t_range, trajectory=traj)(t)\n",
    "        out = get_generation(model, f, config.data, sample=sample_generations)\n",
    "        # export\n",
    "        current_out_path = f\"{stretching_path}/{filename}_{t_f}.wav\"\n",
    "        out_raw = transforms.invert(out)\n",
    "        if out_raw.ndim < 1:\n",
    "            out_raw = out_raw.unsqueeze(0)\n",
    "        print(current_out_path, \"=>\", out_raw.shape)\n",
    "        torchaudio.save(current_out_path, out_raw, sample_rate=config.get('sr', 44100))\n",
    "\"\"\"\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8900fe71",
   "metadata": {},
   "source": [
    "## Feedback markov chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b22698c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feedbacking generations: 100%|██████████████| 8192/8192 [01:48<00:00, 75.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-14.9586) tensor(13.2884) tensor(-0.0547) tensor(5.0598)\n",
      "samples/additive_vae/feedback/random_0.wav => torch.Size([2, 131072])\n",
      "samples/additive_vae/feedback/random_1.wav => torch.Size([2, 131072])\n",
      "samples/additive_vae/feedback/random_2.wav => torch.Size([2, 131072])\n",
      "samples/additive_vae/feedback/random_3.wav => torch.Size([2, 131072])\n",
      "samples/additive_vae/feedback/random_4.wav => torch.Size([2, 131072])\n",
      "samples/additive_vae/feedback/random_5.wav => torch.Size([2, 131072])\n",
      "samples/additive_vae/feedback/random_6.wav => torch.Size([2, 131072])\n",
      "samples/additive_vae/feedback/random_7.wav => torch.Size([2, 131072])\n",
      "samples/additive_vae/feedback/random_8.wav => torch.Size([2, 131072])\n",
      "samples/additive_vae/feedback/random_9.wav => torch.Size([2, 131072])\n",
      "samples/additive_vae/feedback/random_10.wav => torch.Size([2, 131072])\n",
      "samples/additive_vae/feedback/random_11.wav => torch.Size([2, 131072])\n",
      "samples/additive_vae/feedback/random_12.wav => torch.Size([2, 131072])\n",
      "samples/additive_vae/feedback/random_13.wav => torch.Size([2, 131072])\n",
      "samples/additive_vae/feedback/random_14.wav => torch.Size([2, 131072])\n",
      "samples/additive_vae/feedback/random_15.wav => torch.Size([2, 131072])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 16 is out of bounds for dimension 0 with size 16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sn/3rsf7h6j1cd46_rxqs61nm_00000gn/T/ipykernel_29894/3674068256.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mx_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mcurrent_out_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{feedback_path}/random_{n}.wav\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mout_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 16 is out of bounds for dimension 0 with size 16"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "n_particles = 16\n",
    "len_gen = 512\n",
    "n_iter = 8192\n",
    "input_dim = model.encoder.input_size\n",
    "sample_generations = False\n",
    "sample_latent = True\n",
    "\n",
    "feedback_path = f\"{out_path}/feedback\"\n",
    "checkdir(feedback_path)\n",
    "\n",
    "x = torch.randn(n_particles, *tuple(input_dim))\n",
    "with torch.no_grad():\n",
    "    for n in tqdm.tqdm(range(n_iter), desc=\"feedbacking generations\", total=n_iter):\n",
    "        z = model.encode(x)\n",
    "        if isinstance(z, dist.Distribution):\n",
    "            z = z.sample() if sample_latent else z.mean\n",
    "        x = model.decode(z)\n",
    "        if isinstance(x, dist.Distribution):\n",
    "            x = x.sample() if sample_latent else x.mean\n",
    "print(z.min(), z.max(), z.mean(), z.std())\n",
    "\n",
    "for n in range(n_iter):\n",
    "    x_tmp = x[n].repeat(len_gen, 1)\n",
    "    current_out_path = f\"{feedback_path}/random_{n}.wav\"\n",
    "    out_raw = transforms.invert(x_tmp)\n",
    "    if out_raw.ndim < 1:\n",
    "        out_raw = out_raw.unsqueeze(0)\n",
    "    print(current_out_path, \"=>\", out_raw.shape)\n",
    "    torchaudio.save(current_out_path, out_raw, sample_rate=config.get('sr', 44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a5934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
